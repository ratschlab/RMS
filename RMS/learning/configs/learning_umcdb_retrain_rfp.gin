
parse_gin_args.gin_configs = {

    # ================= INPUT PATHS ======================================================================================================

    # Features / labels to use for the two data-sets
    "umc_ml_input_dir": "../../data/ml_input/umcdb_features_OLD",  # OLD DATA-SET
    "bern_ml_input_dir": "../../data/ml_input/hirid2_features",

    # HiRID v8 schema dict
    "hirid_v8_dict": "../../data/misc/hirid_v8_schema.pickle",

    # ML dataset to use for the two data-sets
    "umc_ml_dset_dir": "../../data/ml_dset/umcdb_dset_OLD", # OLD DATA-SET
    "bern_ml_dset_dir": "../../data/ml_dset/hirid2_dset",
    
    # Imputed data to use for the two data-sets
    "umc_imputed_dir": "../../data/imputed/noimpute_umcdb_OLD",  # OLD DATA-SET
    "bern_imputed_dir": "../../data/imputed/noimpute_hirid2",

    # Path of batch map for the two data-sets
    "umc_pid_batch_map_path": "../../data/exp_design/umcdb_chunking.pickle", # OLD DATA-SET
    "bern_pid_batch_map_path": "../../data/exp_design/hirid2_chunking_100.pickle", 

    # Temporal split descriptor for the two data-sets
    "umc_temporal_data_split_path": "../../data/exp_design/random_splits_umcdb.pickle",
    "bern_temporal_data_split_path": "../../data/exp_design/temp_splits_hirid2.pickle",

    # Variable encoding dictionary of HiRID-II
    "varencoding_dict_path": "../../data/misc/meta_varencoding_map_v8.pickle",

    # In case variables shall be restricted, list to use
    "var_restrict_path": "../../data/var_lists/resp_fail/resp_fail_model_prefix_20vars.txt",

    # In case features shall be restricted, list to use
    "feat_restrict_path": None,

    # ================= OUTPUT PATHS ======================================================================================================

    # Path where to store predictions
    "output_dir": "../../data/predictions",

    # Logging directory
    "log_dir": "/cluster/home/mhueser/log_files/icu_score_resp",

    # ================= ARGUMENTS ======================================================================================================

    # Endpoint to process
    "endpoint": "resp",

    # External validation mode (validation, retrain, internal), for validation we train the model on the train/val split of the HIRID-II
    # database, and test it on the test set of the split in the UMC database. For retrain the model is trained on the
    # training and validation data-set of the UMC database, and applied on the same test-set of the UMC database. 
    "ext_val_mode": "retrain",
    
    # Hyperparameter search grid for LightGBM

    # Large sample problems (second grid), small sample problem (first grid)
    "GBM_HP_GRID": {"n_estimators": [5000], "num_leaves": [32], "learning_rate": [0.1], 
                    "colsample_bytree": [0.5], "rowsample_bytree": [0.5]},

    # Constant hyperparameters for LightGBM (1000 for large-sample problems, 50 for small sample problems)
    "lgbm_min_child_samples": 1000,
    
    "lgbm_is_unbalanced": False,

    # HP grid for ExtraTrees'
    "ETREES_HP_GRID": {"n_estimators": [100,1000,10000]},

    # HP grid for random forest
    "RFOREST_HP_GRID": {"n_estimators": [200,400,600,800]},     

    # Hyperparameter search grid for a single decision tree
    "TREE_GRID": {"n_estimators": [1], "num_leaves": [32], "learning_rate": [0.05]},

    # Hyperparameter search grid for logistic regression
    "LR_GRID": {"alpha": [1.0,0.1,0.01,0.001,0.0001,0.00001]},

    # Standard alpha to use for log-reg
    "logreg_alpha": 0.1,

    # HP search grid for MLP classifier
    "MLP_GRID": {"hidden_layer_size": [10,20,50,100], "learning_rate": [0.001], "alpha": [0.01,0.001,0.0001,0.00001]},

    # Machine learning model to use (tree or lightgbm)
    "ml_model": "lightgbm",

    # Split to use in the UMCDB and Bern data-sets, relevant for some external validation modes
    "umc_split_key": "random_1",    
    "bern_split_key": "temporal_1",

    # Mean-impute missing values (needed for Sklearn models)
    "mean_impute_nans": False,

    # Scale-data (needed for LogReg/MLP)
    "scale_encode_data": False,

    # Only encode data (needed for Sklearn models)
    "encode_data": False,

    # Filter out constant features
    "filter_low_variance": False,

    # Split name if random split should be loaded?
    "special_development_split": "NONE",

    # Label to fit in binary classification
    "label_key": "Label_WorseStateFromZeroOrOne0To24Hours",

    # Objective for classification task (binary or multiclass)
    "clf_objective": "binary",

    # Evaluation label (usually identical to training label)
    "eval_label_key": None,

    # Feature column set to use
    "column_set": "retrain_rmsRFP",
    
    # Is standard-scaled data used?
    "scaled_data": False,

    # Should we produce predictions at all time-points?
    "pred_everywhere": True,

    # Attach circEWS labels as features?
    "attach_circews_labels": False,

    # Restrict the variable set to get a compact model?
    "restrict_variables": True,

    # Restrict the features to a pre-specified list
    "restrict_feats": False,

    # HDF compression settings
    "hdf_comp_alg": "blosc:lz4",
    "hdf_comp_level": 5,

    # Debugging

    # Do not write to the file-system
    "debug_mode": False,
    "random_state": 2022,
    "profile_report": False,
    "systrace_mode": False,

    # Execution mode
    "run_mode": "INTERACTIVE",

    # Should one tree be plotted and saved?
    "plot_tree": False,
    
    # Library settings for GBM
    "use_xgboost": False,
    "use_catboost": False,

    # Use a test set from another split than the main split?
    "special_test_set": "NONE",

    # Use only current-value features?
    "only_plain_vars": False,

    # Use only basic features (for variable selection)
    "only_base_feats": False,

    # Should forward or backward variable selection be run?
    "select_variables_forward": False,
    "select_variables_backward": False,

    # Run training on smaller subsample?
    "negative_subsampling": False,
    "50percent_sample": False,
    "50percent_sample_train": False,
    "25percent_sample": False,
    "25percent_sample_train": False,
    "10percent_sample_train": False,
    "5percent_sample_train": False,
    "2percent_sample_train": False,
    "1percent_sample_train": False,             
    "20percent_sample": False,
    "10percent_sample": False,
    "5percent_sample": False,
    "1percent_sample": False,
    "verysmall_sample": False,

    # Remove time since admission
    "remove_reltime": False,

    # Restrict to some year in the training set?
    "special_year": -1,

    # Save ML inputs to disk
    "save_ml_inputs": False,

    # Evaluation metric (auprc for binary, multiclass otherwise)
    "custom_eval_metric": "auprc",

    # Class weight (None for binary tasks, 'balanced' for multiclass tasks
    "class_weight": None,

    # Refit full matrix
    "refit_with_val_data": False,
    
    # Random sub-sampling ratio for the special year mode
    "special_year_sample": 0.5,

    # Training set ratio of PIDs to use from selected year?
    "special_year_train_ratio": 0.75,

    # Static columns and different types
    "static_cols": ["static_Age","static_Sex"],
    "static_cols_raw": ["Age","Sex","Emergency",'Height'],
    "static_cols_without_encode": ["Age","Height","Emergency"],
    "static_cols_one_hot_encode": [],
    "static_cols_one_hot_encode_str": ["Sex"],

    # Unique values of categorical columns
    "unique_values_cat": { "PatGroup": [113,116,5,115,114,117,-1,118],
                           "APACHECode": [5,6,3,0,2,10,11,8,7,4],
                           "Discharge": [2,4],
                           "Euroscores": [17,16,18,19,20,15,21,22,14,24,23],
                           "Surgical": [3,0,1],
                           "Sex": ["M","F","U"] },

    "str_to_int_sex": {"M": 0, "F": 1, "U": 2},

    # Filter low variance data threshold
    "filter_low_variance": False,
    "std_eps": 1e-5,

    # Should a univariate test be run?
    "univariate_test": False,    

    # Remove static variables
    "ablate_static": False,

    # Remove measurement-based features
    "ablate_measurement": False,

    # Remove multi-resolution features
    "ablate_multiresolution": False,

    # Remove instability based features
    "ablate_instability": False,

    # Multi-resolution only short features
    "multires_only_short": False,

    # Multi-resolution plus med features
    "multires_plus_med": False,

    # Multi-resolution plus long features
    "multires_plus_long": False,

    # Multi-resolution plus longest features
    "multires_plus_longest": False,

    # Multi-resolution only longest features
    "multires_only_longest": False,

    # Only location summary
    "summary_loc": False,

    # Only location+trend summary
    "summary_loc_trend": False,

    # All summary functions
    "summary_all": False,

    # Bowen HMM features
    "add_bowen": False,
    "only_bowen": False

}
